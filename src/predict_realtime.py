"""
Real-time Waste Classification using Webcam + YOLO Object Detection
Ph√¢n lo·∫°i r√°c th·∫£i real-time qua webcam v·ªõi nh·∫≠n di·ªán v·∫≠t th·ªÉ

Usage:
    python src/predict_realtime.py
    python src/predict_realtime.py --model outputs/models/baseline_final.keras
    python src/predict_realtime.py --camera 1 --confidence 0.5
"""

import os
import sys
import argparse
import time
from datetime import datetime
import numpy as np
import cv2
from PIL import Image

# Import config
sys.path.insert(0, os.path.dirname(__file__))
from config import CLASS_NAMES, IMG_SIZE, MODELS_DIR, SCREENSHOTS_DIR

# T·∫Øt warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import warnings
warnings.filterwarnings('ignore')

try:
    import tensorflow as tf
    from tensorflow import keras
except ImportError:
    print("‚ùå Kh√¥ng t√¨m th·∫•y TensorFlow!")
    print("üí° C√†i ƒë·∫∑t: pip install tensorflow")
    sys.exit(1)

try:
    import cv2
except ImportError:
    print("‚ùå Kh√¥ng t√¨m th·∫•y OpenCV!")
    print("üí° C√†i ƒë·∫∑t: pip install opencv-python")
    sys.exit(1)

try:
    from ultralytics import YOLO
except ImportError:
    print("‚ùå Kh√¥ng t√¨m th·∫•y Ultralytics YOLO!")
    print("üí° C√†i ƒë·∫∑t: pip install ultralytics")
    sys.exit(1)


class RealtimePredictor:
    """Class x·ª≠ l√Ω d·ª± ƒëo√°n real-time t·ª´ webcam v·ªõi YOLO object detection"""
    
    def __init__(self, model_path, camera_id=0, confidence_threshold=0.5):
        """
        Kh·ªüi t·∫°o predictor
        
        Args:
            model_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn waste classification model
            camera_id: ID c·ªßa camera (default: 0)
            confidence_threshold: Ng∆∞·ª°ng confidence t·ªëi thi·ªÉu (default: 0.5)
        """
        self.model_path = model_path
        self.camera_id = camera_id
        self.confidence_threshold = confidence_threshold
        self.classifier_model = None
        self.yolo_model = None
        self.cap = None
        
        # Color mapping cho t·ª´ng lo·∫°i r√°c (BGR format for OpenCV)
        self.colors = {
            'battery': (0, 165, 255),      # Orange
            'biological': (0, 255, 0),     # Green
            'cardboard': (139, 69, 19),    # Brown
            'clothes': (255, 0, 255),      # Magenta
            'glass': (255, 255, 0),        # Cyan
            'metal': (128, 128, 128),      # Gray
            'paper': (255, 255, 255),      # White
            'plastic': (0, 0, 255),        # Red
            'shoes': (255, 0, 0),          # Blue
            'trash': (0, 0, 0)             # Black
        }
        
        # FPS tracking
        self.fps_history = []
        self.max_fps_history = 30
        
    def load_models(self):
        """Load c·∫£ classification model v√† YOLO model"""
        # Load waste classifier
        print(f"üîÑ ƒêang load waste classification model: {self.model_path}")
        
        if not os.path.exists(self.model_path):
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y model: {self.model_path}")
            print(f"\nüí° C√°c model c√≥ s·∫µn:")
            if os.path.exists(MODELS_DIR):
                for f in os.listdir(MODELS_DIR):
                    if f.endswith('.keras'):
                        print(f"   - {os.path.join(MODELS_DIR, f)}")
            return False
        
        try:
            self.classifier_model = keras.models.load_model(self.model_path)
            print(f"‚úÖ ƒê√£ load waste classifier th√†nh c√¥ng!")
        except Exception as e:
            print(f"‚ùå L·ªói khi load model: {e}")
            return False
        
        # Load YOLO model
        print(f"üîÑ ƒêang load YOLO model (YOLOv8n)...")
        try:
            self.yolo_model = YOLO('yolov8n.pt')  # Nano model, nh·∫π nh·∫•t
            print(f"‚úÖ ƒê√£ load YOLO th√†nh c√¥ng!")
        except Exception as e:
            print(f"‚ùå L·ªói khi load YOLO: {e}")
            print("üí° YOLO s·∫Ω t·ª± ƒë·ªông download l·∫ßn ƒë·∫ßu ch·∫°y (~6MB)")
            return False
        
        return True
    
    def setup_camera(self):
        """Thi·∫øt l·∫≠p camera"""
        print(f"üìπ ƒêang m·ªü camera {self.camera_id}...")
        
        self.cap = cv2.VideoCapture(self.camera_id)
        
        if not self.cap.isOpened():
            print(f"‚ùå Kh√¥ng th·ªÉ m·ªü camera {self.camera_id}")
            print("\nüí° Th·ª≠:")
            print("   - Ki·ªÉm tra camera c√≥ ho·∫°t ƒë·ªông kh√¥ng")
            print("   - Th·ª≠ camera kh√°c: --camera 1")
            print("   - Ki·ªÉm tra quy·ªÅn truy c·∫≠p camera")
            return False
        
        # Set resolution
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        
        print(f"‚úÖ Camera ƒë√£ s·∫µn s√†ng!")
        return True
    
    def preprocess_crop(self, crop_bgr):
        """
        Ti·ªÅn x·ª≠ l√Ω crop c·ªßa object - CHU·∫®N H√ìA GI·ªêNG predict.py
        
        Args:
            crop_bgr: Crop t·ª´ frame (BGR format)
            
        Returns:
            numpy array ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        """
        # Convert BGR sang RGB
        crop_rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)
        
        # Chuy·ªÉn sang PIL Image (gi·ªëng predict.py)
        pil_img = Image.fromarray(crop_rgb)
        
        # Resize v·ªõi PIL (gi·ªëng predict.py)
        pil_img = pil_img.resize(IMG_SIZE, Image.LANCZOS)
        
        # Convert sang array v√† normalize
        img_array = np.array(pil_img, dtype='float32') / 255.0
        
        # Th√™m batch dimension
        img_array = np.expand_dims(img_array, axis=0)
        
        return img_array
    
    def classify_crop(self, crop):
        """
        Classify m·ªôt crop c·ªßa object
        
        Args:
            crop: Crop t·ª´ frame (BGR)
            
        Returns:
            (class_name, confidence)
        """
        # Ti·ªÅn x·ª≠ l√Ω v·ªõi PIL (chu·∫©n h√≥a)
        processed = self.preprocess_crop(crop)
        
        # D·ª± ƒëo√°n
        predictions = self.classifier_model.predict(processed, verbose=0)
        probabilities = predictions[0]
        
        # L·∫•y top 1
        top_idx = np.argmax(probabilities)
        class_name = CLASS_NAMES[top_idx]
        confidence = float(probabilities[top_idx])
        
        return class_name, confidence
    
    def detect_and_classify(self, frame):
        """
        Detect objects v·ªõi YOLO v√† classify t·ª´ng object
        
        Args:
            frame: Frame t·ª´ camera (BGR)
            
        Returns:
            List of detections: [(x1, y1, x2, y2, class_name, confidence), ...]
        """
        # YOLO detection
        results = self.yolo_model(frame, verbose=False)[0]
        
        detections = []
        
        # X·ª≠ l√Ω t·ª´ng detection
        for box in results.boxes:
            # L·∫•y bounding box coordinates
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            
            # ƒê·∫£m b·∫£o bounding box h·ª£p l·ªá
            h, w = frame.shape[:2]
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(w, x2), min(h, y2)
            
            # Ki·ªÉm tra k√≠ch th∆∞·ªõc t·ªëi thi·ªÉu (√≠t nh·∫•t 50x50 pixels)
            if (x2 - x1) < 50 or (y2 - y1) < 50:
                continue
            
            # Crop object
            crop = frame[y1:y2, x1:x2]
            
            # Classify crop
            waste_class, waste_conf = self.classify_crop(crop)
            
            # Ch·ªâ gi·ªØ l·∫°i n·∫øu confidence ƒë·ªß cao
            if waste_conf >= self.confidence_threshold:
                detections.append((x1, y1, x2, y2, waste_class, waste_conf))
        
        return detections
    
    def draw_detections(self, frame, detections, fps):
        """
        V·∫Ω bounding boxes v√† labels l√™n frame
        
        Args:
            frame: Frame g·ªëc
            detections: List of (x1, y1, x2, y2, class_name, confidence)
            fps: FPS hi·ªán t·∫°i
            
        Returns:
            Frame ƒë√£ v·∫Ω
        """
        height, width = frame.shape[:2]
        
        # V·∫Ω FPS v√† title ·ªü g√≥c tr√™n
        cv2.rectangle(frame, (10, 10), (width - 10, 80), (0, 0, 0), -1)
        cv2.rectangle(frame, (10, 10), (width - 10, 80), (100, 100, 100), 2)
        
        cv2.putText(frame, "WASTE CLASSIFIER + YOLO", (20, 40),
                   cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 255, 255), 2)
        cv2.putText(frame, f"FPS: {fps:.1f} | Objects: {len(detections)}", (20, 65),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)
        
        # V·∫Ω t·ª´ng detection
        for (x1, y1, x2, y2, class_name, confidence) in detections:
            color = self.colors.get(class_name, (255, 255, 255))
            
            # V·∫Ω bounding box
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)
            
            # V·∫Ω label background
            label = f"{class_name.upper()}: {confidence * 100:.1f}%"
            (label_w, label_h), baseline = cv2.getTextSize(
                label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2
            )
            
            # Label box
            cv2.rectangle(frame, 
                         (x1, y1 - label_h - 10), 
                         (x1 + label_w + 10, y1), 
                         color, -1)
            
            # Label text
            cv2.putText(frame, label, (x1 + 5, y1 - 5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)
            
            # V·∫Ω confidence bar b√™n trong box
            bar_height = 10
            bar_width = int((x2 - x1) * confidence)
            cv2.rectangle(frame, 
                         (x1, y2 - bar_height), 
                         (x1 + bar_width, y2), 
                         color, -1)
        
        # Instructions
        cv2.putText(frame, "Press 'q' to quit | 's' to screenshot", 
                   (20, height - 20),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)
        
        return frame
    
    def save_screenshot(self, frame):
        """
        L∆∞u screenshot
        
        Args:
            frame: Frame c·∫ßn l∆∞u
        """
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
        os.makedirs(SCREENSHOTS_DIR, exist_ok=True)
        
        # T·∫°o filename v·ªõi timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"screenshot_{timestamp}.jpg"
        filepath = os.path.join(SCREENSHOTS_DIR, filename)
        
        # L∆∞u
        cv2.imwrite(filepath, frame)
        print(f"\nüì∏ Screenshot saved: {filepath}")
    
    def run(self):
        """Main loop - ch·∫°y real-time prediction v·ªõi YOLO"""
        
        # Load models
        if not self.load_models():
            return
        
        # Setup camera
        if not self.setup_camera():
            return
        
        print("\n" + "="*60)
        print("üé• REAL-TIME WASTE CLASSIFICATION + YOLO")
        print("="*60)
        print("üìπ Camera ƒëang ch·∫°y...")
        print("ü§ñ YOLO ƒëang detect objects...")
        print("‚å®Ô∏è  Nh·∫•n 'q' ƒë·ªÉ tho√°t")
        print("‚å®Ô∏è  Nh·∫•n 's' ƒë·ªÉ ch·ª•p ·∫£nh")
        print("="*60 + "\n")
        
        try:
            frame_count = 0
            
            while True:
                start_time = time.time()
                
                # ƒê·ªçc frame
                ret, frame = self.cap.read()
                if not ret:
                    print("‚ùå Kh√¥ng th·ªÉ ƒë·ªçc frame t·ª´ camera")
                    break
                
                frame_count += 1
                
                # Detect v√† classify (m·ªói 2 frames ƒë·ªÉ tƒÉng FPS)
                if frame_count % 2 == 0:
                    detections = self.detect_and_classify(frame)
                else:
                    # Gi·ªØ detections c≈©
                    if frame_count == 1:
                        detections = []
                
                # T√≠nh FPS
                elapsed = time.time() - start_time
                fps = 1.0 / elapsed if elapsed > 0 else 0
                self.fps_history.append(fps)
                if len(self.fps_history) > self.max_fps_history:
                    self.fps_history.pop(0)
                avg_fps = sum(self.fps_history) / len(self.fps_history)
                
                # V·∫Ω detections
                display_frame = self.draw_detections(frame, detections, avg_fps)
                
                # Hi·ªÉn th·ªã
                cv2.imshow('Waste Classifier + YOLO - Real-time', display_frame)
                
                # X·ª≠ l√Ω keyboard
                key = cv2.waitKey(1) & 0xFF
                
                if key == ord('q'):
                    print("\nüëã Tho√°t...")
                    break
                elif key == ord('s'):
                    self.save_screenshot(display_frame)
        
        except KeyboardInterrupt:
            print("\n\nüëã Tho√°t (Ctrl+C)...")
        
        finally:
            # Cleanup
            if self.cap:
                self.cap.release()
            cv2.destroyAllWindows()
            print("‚úÖ ƒê√£ ƒë√≥ng camera v√† c·ª≠a s·ªï")


def main():
    parser = argparse.ArgumentParser(
        description='Real-time waste classification using webcam + YOLO object detection',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
V√≠ d·ª•:
  python src/predict_realtime.py
  python src/predict_realtime.py --model outputs/models/baseline_final.keras
  python src/predict_realtime.py --camera 1 --confidence 0.6
  
Features:
  ‚úÖ YOLO object detection - Detect nhi·ªÅu v·∫≠t th·ªÉ
  ‚úÖ Bounding boxes - Vi·ªÅn xung quanh v·∫≠t th·ªÉ
  ‚úÖ Real-time classification - Ph√¢n lo·∫°i t·ª´ng v·∫≠t th·ªÉ
  ‚úÖ High accuracy - Preprocessing chu·∫©n v·ªõi PIL
  
Keyboard shortcuts:
  q - Quit
  s - Save screenshot
        """
    )
    
    parser.add_argument('--model', type=str,
                       default=os.path.join(MODELS_DIR, 'mobilenetv2_final.keras'),
                       help='ƒê∆∞·ªùng d·∫´n ƒë·∫øn waste classification model')
    parser.add_argument('--camera', type=int, default=0,
                       help='Camera device ID (default: 0)')
    parser.add_argument('--confidence', type=float, default=0.5,
                       help='Minimum confidence threshold (default: 0.5)')
    
    args = parser.parse_args()
    
    # T·∫°o predictor v√† ch·∫°y
    predictor = RealtimePredictor(
        model_path=args.model,
        camera_id=args.camera,
        confidence_threshold=args.confidence
    )
    
    predictor.run()


if __name__ == "__main__":
    main()
